1.标准的多头注意力中每个头都有独立的Key和Value参数，而GQA是通过吧query分组，同一组q头共享同一组k-v参数；
可以减少k-v的数量来减少计算运行时的缓存所占用的内存，并且保证了每个q之间的独立性，这样做的好处就是能提高运算速度。

2.Llama3设计了一套自己的缩放定律（scaling laws）来确定旗舰模型的尺寸，首先建立了计算最优模型在下游任务上的负对数似然比与FLOP之间的联系，在不同计算预算下训练多个不同参数的小模型并绘制了每个计算预算的Scaling law IsoFLOPs curves；
用二阶的多项式来拟合损失值并把找到每个抛物线的最低点来作为最优的模型，通过这些最低点拟合出计算预算与最优训练词元数之间的指数关系，再外推在旗舰模型的3.8 × 10^25 FLOPs训练预算下最优模型大小约为 402B 参数；
最后考虑到IsoFLOPs曲线在最小值附近变得更加平坦，表明旗舰模型的性能对于模型大小和训练token数之间的权衡的微小变化是具有鲁棒性的，因此最后训练一个具有405B参数的旗舰模型。

3.四种 parallelism 的主要特点:
Tensor Parallelism (TP)：将权重张量切分成多重的数据块分到不同设备上，每个设备计算一部分。
Pipeline Parallelism ()):	将模逐按层垂直划分成多个阶段，不同设可以并行备处理不同阶段。
Context Parallelis(CP):)	将的上下文内容拆序成列处理，输入减少长时序记忆瓶颈压力。
Data Parallem (lis:使用：使用FSDP将模型、优化器和梯度进行分片，同时实现数据并行在以便可以在多个GPU上并行处理数据，最后在每一步训练完成后同步各个计算结果。

4.Llama3训练阶段分为初始预训练，长上下文预训练，退火三个
（1）初始预训练阶段先使用小批次的数据进行训练来提高训练稳定性，随后逐渐增加批次大小来提高效率，这样的训练方式非常稳定，实验证明损失峰很少，并且不需要干预措施来纠正模型训练的分歧；在数据组合上对数学数据进行了上采样提高了模型的数学推理性能，对预训练数据中被识别为质量较低的子集进行了下采样。
（2）由于自注意力层中的计算在序列长度上呈平方增长，所以没有一开始就使用长序列训练。而是在长上下文预训练则在长序列上进行训练，分6个阶段逐步增加上下文长度，极大的提升了训练的效率，减少了无效计算损失。
（3）最后在保持上下文长度不变的条件下进行退火处理，将学习率逐步调整为0，同时用上采样高质量代码和数学数据；此时学习率很低，能够对更高质量数据进行精细的分析和理解，避免了高质量数据被海量低质量数据淹没的情况，从而显著提升在编程、数学等特定领域的性能，同时不会破坏之前学习到的通用能力。

5.定义聊天协议的最大目的在于让模型理解人类指令和执行会话任务，因为Llama3可能需要生成多个消息并在一个对话轮内发送到不同位置。
为此他们设计了一个新的聊天协议，使用header tokens和termination tokens，header tokens用于指示会话中每个消息的来源和目的地，termination tokens指示人与人工智能应该在什么时间轮流说话。

6.RM的作用如下：
辅助进行拒绝采样（RS），对于人工标注时收集到的每个提示词，采样K个输出并使用奖励模型选择最佳而且和Bia et al相一致的提示词。（4.2.2）（2）
为SFT提供拒绝采样数据：在SFT中奖励模型对人工标注的提示词进行拒绝采样，结合该拒绝采样数据和其他数据，对目标tokens使用标准交叉熵损失对预训练的语言模型进行微。。（4.1.3）# （3）对数据处理的之后每个样本的质量进行打分，把RM分数的前四分位数的数据认为是高质量的；因为RM和Llama的评分有很高的不一致率，所以他们发现结合这些信号在Llama3的内部测试集上会产生了最好的召回率。（4.2.DP)DP)
