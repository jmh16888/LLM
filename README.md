# LLM
这是AI派第二轮的面试题目，包含我对Llama技术报告的思考与理解\n
1.标准的多头注意力中每个头都有独立的Key和Value参数，而GQA是通过吧query分组，同一组q头共享同一组k-v参数，可以减少k-v的数量来减少计算运行时的缓存所占用的内存，并且保证了每个q之间的独立性，这样做的好处就是能提高运算速度。\n
2.Llama3设计了一套自己的缩放定律（scaling laws）来确定旗舰模型的尺寸，首先建立了计算最优模型在下游任务上的负对数似然比与FLOP之间的联系，在不同计算预算下训练多个不同参数的小模型并绘制了每个计算预算的Scaling law IsoFLOPs curves。
用二阶的多项式来拟合损失值并把找到每个抛物线的最低点来作为最优的模型，通过这些最低点拟合出计算预算与最优训练词元数之间的指数关系，再外推在旗舰模型的3.8 × 10^25 FLOPs训练预算下最优模型大小约为 402B 参数；
最后考虑到IsoFLOPs曲线在最小值附近变得更加平坦，表明旗舰模型的性能对于模型大小和训练token数之间的权衡的微小变化是具有鲁棒性的，因此最后训练一个具有405B参数的旗舰模型。
3.
